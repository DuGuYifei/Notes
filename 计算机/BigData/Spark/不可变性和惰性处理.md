# 不可变性和惰性处理

1. 不可变性（Immutability）和惰性处理（Lazy Processing）是 Spark 中的两个重要概念。
2. 在大多数编程语言中，变量通常是可变的，可以随时更改其值。但是在 Spark 中，**数据帧（DataFrames）是不可变的，即定义后不能修改**。
3. 不可变性是函数式编程的一个重要组成部分。Spark 设计为使用不可变对象，这样可以在多个并发组件之间共享数据，而无需担心并发数据对象的问题。
4. 在 Spark 中，对数据帧进行操作时，实际上并没有立即执行操作，而是**延迟执行（惰性处理）。只有在执行行动（Action）时，才会触发实际的计算和数据处理操作**。
5. 通过延迟处理，Spark 可以根据最优算法和操作顺序来执行数据处理操作，从而提高效率。


举个惰性处理的例子：

```python
data = [1, 2, 3, 4, 5]
df = spark.createDataFrame([(num,) for num in data], ["numbers"])

# 定义转换操作，但不立即执行：
# 使用 filter 方法过滤出所有偶数。然后，使用 select 方法对过滤后的数据集进行乘以2的操作
transformed_df = df.filter(df.numbers % 2 == 0).select(df.numbers * 2)

# 执行最终的计算操作：
# 在这一步中，我们对转换后的数据集执行了聚合操作 groupBy().sum()，并最终调用了 collect() 方法来触发实际的计算。collect() 方法是一个行动操作，它会触发 Spark 执行整个操作执行计划并返回结果。
result = transformed_df.groupBy().sum().collect()[0][0]
```