# 分区和ID和惰性处理

1. 分区：Spark将DataFrame划分为多个分区，也称为数据块。这些分区可以根据Spark集群的类型自动定义、扩大或缩小，并且可能会有很大的差异。通常情况下，尽量保持分区大小相等。我们假设每个分区是独立处理的，这是Spark提供性能和横向扩展能力的一部分。如果Spark节点无需争夺资源，也无需与其他Spark节点协商答案，它就可以可靠地调度处理以获得最佳性能。

2. 惰性处理：在Spark中，任何转换操作都是惰性的，更像是一个指令而不是立即执行的操作。它定义了对DataFrame应该进行的操作，而不是立即执行它。在Spark中，大多数操作实际上都是转换操作，包括.withColumn()、.select()、.filter()等等。定义的转换操作只有在运行Spark动作时才会执行。这包括.count()、.write()等需要执行转换操作才能正确获得结果的操作。Spark可以重新排序转换操作以获得最佳性能。通常情况下，这并不会引起注意，但有时可能会导致意外行为，例如在其他转换操作完成之前没有添加ID。这并不会导致问题，但如果不知道预期结果，数据可能会显得不寻常。

3. 添加ID：关系数据库通常具有用于标识行的字段，无论是用于实际关系引用还是用于数据标识。这些ID通常是递增的整数，并且最重要的是唯一的。然而，这些ID在并行处理方面并不是很适用。由于这些值是按顺序分配的，如果存在多个工作节点，它们必须都参考一个共同的源以获取下一个条目。在单个服务器环境中，这没问题，但在Spark等分布式平台中，这会产生一些不必要的瓶颈。让我们看看如何在Spark中生成ID。

4. 单调递增的ID：Spark有一个内置函数[monotonically_increasing_id()](monotonically_increasing_id.md)，用于生成递增且唯一的整数ID。这些ID不一定是连续的，之间可能会有很大的间隔。与普通的关系型ID不同，Spark的ID是完全并行的，每个分区被分配了多达80亿个可分配的ID。注意示例表中的ID字段是整数，递增但不连续。ID是一个64位数字，根据Spark分区进行了分组。每个分组包含84亿个ID，而可用的分组数为21亿，它们之间没有重叠