# 拆分对象和提高导入性能

提高导入性能
我们已经讨论了在使用Spark DataFrames时缓存的好处。现在让我们看看如何提高将数据导入DataFrame的速度。

Spark集群
Spark集群由两种类型的进程组成 - 一个驱动进程和尽可能多的工作进程。驱动程序处理任务分配和来自工作进程的数据结果的汇总。工作进程通常处理Spark作业的实际转换/操作任务。一旦分配了任务，它们在很大程度上独立地操作，并将结果报告给驱动程序。可以有单节点Spark集群（这是我们在本课程中使用的方式），但在实际环境中很少见。有多种运行Spark集群的方法 - 所使用的方法取决于您的具体环境。

导入性能
在导入数据到Spark DataFrames时，重要的是了解集群如何实现作业。该过程因任务类型而异，但可以肯定的是，可用的导入对象越多，集群将能够更好地分配作业。这对于单节点集群可能无关紧要，但对于较大的集群，每个工作进程都可以参与导入过程。换句话说，一个大文件的性能要比许多较小的文件差得多。根据集群的配置，您可能无法处理较大的文件，但可以轻松处理分为较小文件的相同数量的数据。请注意，即使有多个文件，您可以定义单个导入语句。在定义导入文件名时，可以使用任何形式的标准通配符符号。虽然不太重要，但如果对象大小相似，集群的性能要优于具有非常大和非常小对象的组合。

模式
如果您还记得第一章，我们讨论了Spark模式的重要性。在Spark中，定义明确定义的模式可以大大提高导入性能。未定义模式的导入任务需要多次读取数据以推断结构，这在处理大量数据时非常缓慢。Spark可能不会按照您预期的方式定义数据中的对象。Spark模式还提供了导入时的验证。这可以节省数据清洗工作的步骤，并改善整体处理时间。

如何拆分对象
有多种有效的方法可以将对象（主要是文件）拆分为更小的对象。第一种方法是使用内置的操作系统实用工具，如split、cut或awk。使用split的示例使用-l参数指定每个文件的行数