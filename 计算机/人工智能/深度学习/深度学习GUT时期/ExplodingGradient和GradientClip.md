# ExplodingGradient和GradientClipping
梯度爆炸问题
------

比如在LSTM里的反向传播涉及连续乘积运算，如果乘积过大，就会导致梯度爆炸。这种情况下权重的更新变得非常大，导致网络权重变得非常不稳定，甚至导致模型训练失败。

梯度裁剪
----

通过限制梯度的大小来防止过大的权重更新。

在LSTM中，合理的设置学习率和使用梯度裁剪可以帮助控制梯度爆炸问题，从而提高模型的稳定性和性能。