# SGD
简介
--

SGD Stochastic Gradient Descent 随机梯度下降

1.  一个重要参数是learning rate
2.  通常用于监督学习
3.  不保证找到cost function的最小值
4.  可以理解为小批量梯度下降，但在一部分文献中也理解为**一次只有一个样本即小批量的特例**，一次epoch里对所有样本执行一次操作。
5.  单一的全局学习率

SGD with Mometum
----------------

1.  有先前积累的动量（梯度值聚合）（没有单独的自己的加速度）
2.  用单一的全局学习率
3.  动量项有助于平滑优化轨迹，减少优化过程中的振荡，通常可以加速学习过程，并有助于更快地收敛到最优解。